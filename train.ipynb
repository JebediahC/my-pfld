{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.datasets import WLFWDatasets\n",
    "from models.pfld import PFLDInference, AuxiliaryNet\n",
    "from pfld.loss import PFLDLoss\n",
    "from pfld.utils import AverageMeter\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    for arg in vars(args):\n",
    "        s = arg + ': ' + str(getattr(args, arg))\n",
    "        logging.info(s)\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    logging.info('Save checkpoint to {0:}'.format(filename))\n",
    "\n",
    "def str2bool(v: str):\n",
    "    \"\"\"\n",
    "    Transform string arguments to bool type\n",
    "    \"\"\"\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected')\n",
    "\n",
    "def draw_batch(landmarks:np.ndarray, imgs:torch.Tensor, writer:SummaryWriter, tag:str, step:int, draw_landmarks=True):\n",
    "    \"\"\"\n",
    "    绘制图片和landmarks\n",
    "    \"\"\"\n",
    "    if draw_landmarks:\n",
    "        lmrz=landmarks.reshape(landmarks.shape[0], -1,2).cpu().numpy()\n",
    "    img_in=np.asarray(imgs)\n",
    "    img_in=np.transpose(img_in,(0,2,3,1))\n",
    "    img_in = (img_in * 255).astype(np.uint8)\n",
    "    np.clip(img_in, 0, 255)\n",
    "    img_out=np.zeros(img_in.shape,dtype=np.uint8)\n",
    "    for i in range(img_in.shape[0]):\n",
    "        img_i=img_in[i]\n",
    "        img_i=cv2.cvtColor(img_i,cv2.COLOR_BGR2RGB)\n",
    "        if draw_landmarks:\n",
    "            pre_landmark=lmrz[i]*[112,112]\n",
    "            for (x,y) in pre_landmark.astype(np.int32):\n",
    "                cv2.circle(img_i,(x,y),1,(14,184,180),-1)\n",
    "        img_out[i]=img_i\n",
    "\n",
    "    writer.add_images(tag,img_out, step,dataformats='NHWC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, pfld_backbone, auxiliaryNet, criterion, optimizer, epoch ,writer):\n",
    "    losses=AverageMeter()\n",
    "    weighted_loss, loss=None,None\n",
    "    to_draw = True\n",
    "    for img, landmark_gt, attribute_gt, euler_angle_gt in train_loader:\n",
    "        if to_draw and args.show_before_train:\n",
    "            to_draw = False\n",
    "            draw_batch(landmark_gt, img, writer, \"train/ground_true\", epoch, True)\n",
    "        img = img.to(device)\n",
    "        attribute_gt = attribute_gt.to(device)\n",
    "        landmark_gt = landmark_gt.to(device)\n",
    "        euler_angle_gt = euler_angle_gt.to(device)\n",
    "        pfld_backbone = pfld_backbone.to(device)\n",
    "        auxiliaryNet = auxiliaryNet.to(device)\n",
    "        # 计算loss\n",
    "        features, landmarks = pfld_backbone(img)\n",
    "        angle = auxiliaryNet(features)\n",
    "        weighted_loss, loss = criterion(attribute_gt, landmark_gt, euler_angle_gt, angle, landmarks, args.train_batch_size)\n",
    "        # 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.update(loss.item())\n",
    "\n",
    "    return weighted_loss, loss\n",
    "\n",
    "def validate(val_loader, pfld_backbone, auxiliaryNet, criterion, epoch, writer):\n",
    "    to_draw = True\n",
    "    pfld_backbone.eval()\n",
    "    auxiliaryNet.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for img, landmark_gt, attribute_gt, euler_angle_gt in val_loader:\n",
    "            img = img.to(device)\n",
    "            attribute_gt = attribute_gt.to(device)\n",
    "            landmark_gt = landmark_gt.to(device)\n",
    "            euler_angle_gt = euler_angle_gt.to(device)\n",
    "            pfld_backbone = pfld_backbone.to(device)\n",
    "            auxiliaryNet = auxiliaryNet.to(device)\n",
    "\n",
    "            _, landmark = pfld_backbone(img)\n",
    "            loss = torch.mean(torch.sum((landmark_gt - landmark)**2, axis=1)) # l2 distance\n",
    "            losses.append(loss.cpu().numpy()) \n",
    "            # 展示每轮结果\n",
    "            if to_draw and args.show_each_epoch:\n",
    "                to_draw = False\n",
    "                draw_batch(landmark, img.cpu(), writer,\"train/val\", epoch, True)\n",
    "    print(\"===> Evaluate:\")\n",
    "    print('Eval set: Average loss: {:.4f} '.format(np.mean(losses)))\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取参数\n",
    "# parser=argparse.ArgumentParser()\n",
    "# parser.add_argument('-j', '--workers', default=WORKERS, type=int)\n",
    "# parser.add_argument('--devices_id', default=DEVICES_ID, type=str)  #TBD\n",
    "# parser.add_argument('--test_initial', default=TEST_INITIAL, type=str2bool)  #TBDaining\n",
    "# ##  -- optimizer\n",
    "# parser.add_argument('--base_lr', default=BASE_LR, type=int)\n",
    "# parser.add_argument('--weight-decay', '--wd', default=WEIGHT_DECAY, type=float)\n",
    "# # -- lr\n",
    "# parser.add_argument(\"--lr_patience\", default=LR_PATIENCE, type=int)\n",
    "# # -- epoch\n",
    "# parser.add_argument('--start_epoch', default=START_EPOCH, type=int)\n",
    "# parser.add_argument('--end_epoch', default=END_EPOCH, type=int)\n",
    "# # -- snapshot、tensorboard log and checkpoint\n",
    "# parser.add_argument('--snapshot', default=SNAPSHOT, type=str, metavar='PATH')\n",
    "# parser.add_argument('--log_file', default=LOG_FILE, type=str)\n",
    "# parser.add_argument('--tensorboard', default=TENSORBOARD, type=str)\n",
    "# parser.add_argument('--resume', default=RESUME,type=str, metavar='PATH')\n",
    "# # --dataset\n",
    "# parser.add_argument('--dataroot', default=DATA_ROOT, type=str, metavar='PATH')\n",
    "# parser.add_argument('--val_dataroot', default=VAL_DATAROOT, type=str, metavar='PATH')\n",
    "# parser.add_argument('--train_batchsize', default=TRAIN_BATCH_SIZE, type=int)\n",
    "# parser.add_argument('--val_batchsize', default=VAL_BATCH_SIZE, type=int)\n",
    "# args = parser.parse_args()\n",
    "import imp\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.log_file=LOG_FILE\n",
    "        self.base_lr=BASE_LR\n",
    "        self.weight_decay=WEIGHT_DECAY\n",
    "        self.lr_patience=LR_PATIENCE\n",
    "        self.resume=RESUME\n",
    "        self.start_epoch=START_EPOCH\n",
    "        self.end_epoch=END_EPOCH\n",
    "        self.dataroot=DATA_ROOT\n",
    "        self.val_dataroot=VAL_DATAROOT\n",
    "        self.train_batch_size=TRAIN_BATCH_SIZE\n",
    "        self.val_batch_size=VAL_BATCH_SIZE\n",
    "        self.workers=WORKERS\n",
    "        self.show_original_image=SHOW_ORIGINAL_IMAGE\n",
    "        self.show_original_image=SHOW_TEST\n",
    "        self.show_before_train=SHOW_BEFORE_TRAIN\n",
    "        self.show_each_epoch=SHOW_EACH_EPOCH\n",
    "        self.tensorboard=TENSORBOARD\n",
    "        self.snap_shot=SNAPSHOT\n",
    "args=Args() # 在notebook无法得到参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 10:29:02,939] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] log_file: logs/train.logs\n",
      "[2022-03-27 10:29:02,941] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] base_lr: 0.0001\n",
      "[2022-03-27 10:29:02,942] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] weight_decay: 1e-06\n",
      "[2022-03-27 10:29:02,944] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] lr_patience: 40\n",
      "[2022-03-27 10:29:02,951] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] resume: /home/jebediahc/Deep-Learning/my_pfld/snapshot/checkpoint_epoch_59.pth.tar\n",
      "[2022-03-27 10:29:02,953] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] start_epoch: 1\n",
      "[2022-03-27 10:29:02,955] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] end_epoch: 500\n",
      "[2022-03-27 10:29:02,957] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] dataroot: data/train_data/list.txt\n",
      "[2022-03-27 10:29:02,962] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] val_dataroot: data/test_data/list.txt\n",
      "[2022-03-27 10:29:02,967] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] train_batch_size: 32\n",
      "[2022-03-27 10:29:02,971] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] val_batch_size: 32\n",
      "[2022-03-27 10:29:02,974] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] workers: 0\n",
      "[2022-03-27 10:29:02,976] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] show_original_image: True\n",
      "[2022-03-27 10:29:02,978] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] show_test: True\n",
      "[2022-03-27 10:29:02,980] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] show_before_train: True\n",
      "[2022-03-27 10:29:02,984] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] show_each_epoch: True\n",
      "[2022-03-27 10:29:02,987] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] tensorboard: logs/tensorboard_logs\n",
      "[2022-03-27 10:29:02,989] [p2558] [/tmp/ipykernel_2558/1919800256.py:4] [INFO] snap_shot: ./snapshot/\n",
      "[2022-03-27 10:33:08,710] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_59.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.8821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 10:36:47,078] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_60.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.4465 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 10:40:47,481] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_61.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.4246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 10:44:43,191] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_62.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.4139 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 10:48:34,434] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_63.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3985 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 10:52:44,490] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_64.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3974 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 10:56:58,430] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_65.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3849 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:01:26,168] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_66.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3942 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:05:49,231] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_67.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3818 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:10:04,147] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_68.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3933 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:14:23,795] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_69.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3854 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:18:49,824] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_70.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3822 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:23:35,800] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_71.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3754 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:27:58,179] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_72.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3864 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:32:42,958] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_73.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3859 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:36:56,604] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_74.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3779 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:43:52,595] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_75.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.3872 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-27 11:51:44,141] [p2558] [/tmp/ipykernel_2558/1919800256.py:8] [INFO] Save checkpoint to ./snapshot/checkpoint_epoch_76.pth.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Evaluate:\n",
      "Eval set: Average loss: 0.4621 \n"
     ]
    }
   ],
   "source": [
    "# step0: Create dirs and files\n",
    "(log_dir, log_file) = os.path.split(LOG_FILE)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "with open(LOG_FILE,'w'):\n",
    "    pass\n",
    "if not os.path.exists(TENSORBOARD):\n",
    "    os.makedirs(TENSORBOARD)\n",
    "if not os.path.exists(args.snap_shot):\n",
    "    os.makedirs(args.snap_shot)\n",
    "# Step1: parse args and config\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [p%(process)s] [%(pathname)s:%(lineno)d] [%(levelname)s] %(message)s',\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.FileHandler(args.log_file, mode='w'), logging.StreamHandler()]\n",
    ")\n",
    "print_args(args)\n",
    "\n",
    "# Step2: modle, criterion, optimizer, scheduler\n",
    "pfld_backbone = PFLDInference().to(device)\n",
    "auxiliaryNet = AuxiliaryNet().to(device)\n",
    "criterion  = PFLDLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params': pfld_backbone.parameters()},{'params': auxiliaryNet.parameters()}],\n",
    "    lr=args.base_lr,\n",
    "    weight_decay=args.weight_decay\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=args.lr_patience,verbose=True\n",
    ")\n",
    "# 加载检查点模型\n",
    "if args.resume:\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    auxiliaryNet.load_state_dict(checkpoint[\"auxilirynet\"]) # key不太对，但已经保存了，没关系了\n",
    "    pfld_backbone.load_state_dict(checkpoint[\"pfld_backbone\"])\n",
    "    args.start_epoch = checkpoint[\"epoch\"]\n",
    "# Step3: data\n",
    "transform = transforms.Compose([transforms.ToTensor()]) # 数据格式转换\n",
    "wlfwdataset = WLFWDatasets(args.dataroot, transform)\n",
    "dataloader=DataLoader(wlfwdataset, batch_size=args.train_batch_size,shuffle=True, num_workers=args.workers,drop_last=False)\n",
    "wlfw_val_dataset = WLFWDatasets(args.val_dataroot,transform)\n",
    "wlfw_val_dataloader = DataLoader(wlfw_val_dataset, batch_size=args.val_batch_size,shuffle=False,num_workers=0)\n",
    "# Have a look (我觉得最好还是放在预处理部分)\n",
    "# if args.show_original_image:\n",
    "#     pass\n",
    "# Step4: train loop\n",
    "writer=SummaryWriter(args.tensorboard)\n",
    "for epoch in range(args.start_epoch,args.end_epoch):\n",
    "    # 训练，并获得loss\n",
    "    weighted_train_loss, train_loss = train(dataloader, pfld_backbone, auxiliaryNet, criterion, optimizer, epoch, writer)\n",
    "    # 保存检查点\n",
    "    checkpoint_filename=os.path.join(args.snap_shot, \"checkpoint_epoch_\"+str(epoch)+'.pth.tar')\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            'epoch':epoch,\n",
    "            'pfld_backbone':pfld_backbone.state_dict(),\n",
    "            'auxilirynet':auxiliaryNet.state_dict()\n",
    "        },\n",
    "        checkpoint_filename\n",
    "    )\n",
    "    # 验证\n",
    "    val_loss = validate(wlfw_val_dataloader, pfld_backbone,auxiliaryNet, criterion, epoch, writer)\n",
    "    scheduler.step(val_loss)\n",
    "    # loss写入tensorboard\n",
    "    writer.add_scalar('data/weighted_loss', weighted_train_loss, epoch)\n",
    "    writer.add_scalars('data/loss', {'val loss': val_loss,'train loss': train_loss}, epoch)\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddf81131e08de48e6f4fc2676c4c56e33eb75626567de4856de8b67e09518051"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
